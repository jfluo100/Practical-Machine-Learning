---
title: "Practical Machine Learning Course Project"
author: "Junfeng Luo"
date: "January 29, 2019"
output: html_document
---
```
## load packages
```{r}
#install.packages("caret")
#install.packages("randomForest")
#install.packages("e1071")
#install.packages("rpart")
library(caret)
library(randomForest)
library(e1071)
library(rpart)
```

## load datasets
```{r}
training = read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
quiz = read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
```

## breifly summarize the data
```{r}
str(training)
```

## removing variables with missing values
```{r}
training1 = training[, colSums(is.na(training))== 0]
sum(complete.cases(training1))
```

## take a look of the dirstribution of classes
```{r}
percentage <- prop.table(table(training1$classe)) * 100
cbind(freq=table(training1$classe), percentage=percentage)
```

#   freq percentage
# A 5580   28.43747
# B 3797   19.35073
# C 3422   17.43961
# D 3216   16.38977
# E 3607   18.38243

```{r}
barplot(percentage)
```

## after removing variables which all the values are NAs, we still find many variables has very few values, we need to remove those predictors with near zero variance functions removing the columns are not related to making predcition
```{r}
training1 = training1[,-c(1:7)]
x = nearZeroVar(training1)
training1 = training1[,-x]
```

## create training set(70%) and testing set(30%)
## sample the training set to reduce computation
```{r}
training2 = training1[sample(nrow(training1), 2000), ]
inTrain = createDataPartition(training2$classe, p=0.75, list=F)
trainingSet = training2[inTrain,]
testingSet = training2[-inTrain,]
```

## build algorithms:Random Forest, Classification and Regression Trees
```{r}
set.seed(123)
Model_rf = train(classe~.,data = trainingSet, method="rf")
Model_cart = rpart(classe~., data=trainingSet, method="class")
fancyRpartPlot(Model_cart)
prediction_rf = predict(Model_rf, newdata=testingSet)
prediction_cart = predict(Model_cart,newdata = testingSet, type="class")
```

## compute the accuracy
```{r}
confusionMatrix(prediction_rf, testingSet$classe)$overall[1]
sum(prediction_cart==testingSet$classe)/length(testingSet$classe)

```

## use random forest model for predicting final results, process the quiz set same as training set for better outcome
```{r}
quiz = quiz[,colSums(is.na(quiz)) == 0]
quiz = quiz[,-c(1:7)]
result = predict(Model_rf, newdata=quiz)
result
```

Out of sample error of the model = 1- accuracy = 1 -0.93 = 7%  (There exists noise.)